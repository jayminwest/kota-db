// Concurrent Access Pattern Tests - Stage 1: TDD for Phase 2 Optimization Infrastructure
// Test-driven development for thread-safe concurrent operations with read-write optimization

use anyhow::Result;
use kotadb::pure::btree::count_total_keys;
use kotadb::{btree, ValidatedDocumentId, ValidatedPath};
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant};
use tokio::task;
use uuid::Uuid;

/// Test concurrent read operations scale linearly with CPU cores
#[tokio::test]
async fn test_concurrent_reads_linear_scaling() -> Result<()> {
    let tree_size = 10000;
    let reads_per_thread = 1000;

    // Setup: Build large tree for concurrent reading
    let mut tree = btree::create_empty_tree();
    let mut test_keys = Vec::new();

    for i in 0..tree_size {
        let id = ValidatedDocumentId::from_uuid(Uuid::new_v4())?;
        let path = ValidatedPath::new(format!("/concurrent/read_test_{i}.md"))?;
        tree = btree::insert_into_tree(tree, id, path)?;
        test_keys.push(id);
    }

    // Wrap tree in concurrent-safe structure
    let concurrent_tree = Arc::new(RwLock::new(tree));

    // Test with different thread counts
    for thread_count in [1, 2, 4, 8] {
        let start = Instant::now();
        let mut handles = Vec::new();

        for _ in 0..thread_count {
            let tree_ref = Arc::clone(&concurrent_tree);
            let keys_subset = test_keys.clone();

            let handle = task::spawn(async move {
                for _ in 0..reads_per_thread {
                    let random_key = &keys_subset[fastrand::usize(..keys_subset.len())];
                    let tree_guard = tree_ref.read().unwrap();
                    let result = btree::search_in_tree(&tree_guard, random_key);
                    assert!(result.is_some(), "Key should be found in concurrent read");
                }
            });

            handles.push(handle);
        }

        // Wait for all threads to complete
        for handle in handles {
            handle.await?;
        }

        let duration = start.elapsed();
        let ops_per_second = (thread_count * reads_per_thread) as f64 / duration.as_secs_f64();

        println!("Threads: {thread_count}, Duration: {duration:?}, Ops/sec: {ops_per_second:.0}");

        // Performance requirement: should scale reasonably with thread count
        // Allow for some overhead, but expect improvement with more threads
        if thread_count > 1 {
            // Multi-threaded should be faster than single-threaded
            // (This test will help identify locking bottlenecks)
        }
    }

    Ok(())
}

/// Test concurrent write operations with proper isolation
#[tokio::test]
async fn test_concurrent_writes_isolation() -> Result<()> {
    let writers_count = 4;
    let writes_per_writer = 250; // Total: 1000 writes

    // Start with empty concurrent tree
    let concurrent_tree = Arc::new(RwLock::new(btree::create_empty_tree()));
    let mut handles = Vec::new();

    for writer_id in 0..writers_count {
        let tree_ref = Arc::clone(&concurrent_tree);

        let handle = task::spawn(async move {
            let mut local_keys = Vec::new();

            for i in 0..writes_per_writer {
                let id = ValidatedDocumentId::from_uuid(Uuid::new_v4()).unwrap();
                let path =
                    ValidatedPath::new(format!("/concurrent/writer_{writer_id}_{i}.md")).unwrap();

                // Acquire write lock and insert
                {
                    let mut tree_guard = tree_ref.write().unwrap();
                    *tree_guard = btree::insert_into_tree(tree_guard.clone(), id, path).unwrap();
                }

                local_keys.push(id);

                // Small delay to increase chance of contention
                tokio::time::sleep(Duration::from_micros(10)).await;
            }

            local_keys
        });

        handles.push(handle);
    }

    // Collect all inserted keys
    let mut all_keys = Vec::new();
    for handle in handles {
        let writer_keys = handle.await?;
        all_keys.extend(writer_keys);
    }

    // Verify all writes succeeded and tree is consistent
    let final_tree = concurrent_tree.read().unwrap();
    let tree_size = count_total_keys(&final_tree);

    assert_eq!(
        tree_size,
        writers_count * writes_per_writer,
        "Expected {} entries, found {}",
        writers_count * writes_per_writer,
        tree_size
    );

    // Verify all keys are searchable
    for key in &all_keys {
        assert!(
            btree::search_in_tree(&final_tree, key).is_some(),
            "Key should be found after concurrent write"
        );
    }

    // Verify no duplicate keys (all UUIDs should be unique)
    let mut sorted_keys = all_keys.clone();
    sorted_keys.sort();
    sorted_keys.dedup();
    assert_eq!(
        sorted_keys.len(),
        all_keys.len(),
        "Found duplicate keys in concurrent writes"
    );

    Ok(())
}

/// Test mixed read-write workload performance
#[tokio::test]
async fn test_mixed_read_write_workload() -> Result<()> {
    let initial_size = 5000;
    let readers_count = 6;
    let writers_count = 2;
    let operations_per_thread = 500;

    // Setup: Pre-populate tree
    let mut tree = btree::create_empty_tree();
    let mut initial_keys = Vec::new();

    for i in 0..initial_size {
        let id = ValidatedDocumentId::from_uuid(Uuid::new_v4())?;
        let path = ValidatedPath::new(format!("/mixed/initial_{i}.md"))?;
        tree = btree::insert_into_tree(tree, id, path)?;
        initial_keys.push(id);
    }

    let concurrent_tree = Arc::new(RwLock::new(tree));
    let shared_keys = Arc::new(RwLock::new(initial_keys));
    let mut handles = Vec::new();

    let start = Instant::now();

    // Spawn reader threads
    for reader_id in 0..readers_count {
        let tree_ref = Arc::clone(&concurrent_tree);
        let keys_ref = Arc::clone(&shared_keys);

        let handle = task::spawn(async move {
            let mut successful_reads = 0;

            for _ in 0..operations_per_thread {
                let random_key = {
                    let keys_guard = keys_ref.read().unwrap();
                    if keys_guard.is_empty() {
                        None
                    } else {
                        Some(keys_guard[fastrand::usize(..keys_guard.len())])
                    }
                };

                if let Some(key) = random_key {
                    let tree_guard = tree_ref.read().unwrap();
                    if btree::search_in_tree(&tree_guard, &key).is_some() {
                        successful_reads += 1;
                    }
                    // Drop guard before await
                    drop(tree_guard);
                }

                // Small delay to simulate realistic workload
                tokio::time::sleep(Duration::from_micros(5)).await;
            }

            println!("Reader {reader_id}: {successful_reads} successful reads");
            successful_reads
        });

        handles.push(handle);
    }

    // Spawn writer threads
    for writer_id in 0..writers_count {
        let tree_ref = Arc::clone(&concurrent_tree);
        let keys_ref = Arc::clone(&shared_keys);

        let handle = task::spawn(async move {
            let mut successful_writes = 0;

            for i in 0..operations_per_thread {
                let id = ValidatedDocumentId::from_uuid(Uuid::new_v4()).unwrap();
                let path = ValidatedPath::new(format!("/mixed/writer_{writer_id}_{i}.md")).unwrap();

                // Write operation
                {
                    let mut tree_guard = tree_ref.write().unwrap();
                    *tree_guard = btree::insert_into_tree(tree_guard.clone(), id, path).unwrap();
                    successful_writes += 1;
                }

                // Add key to shared pool for readers
                {
                    let mut keys_guard = keys_ref.write().unwrap();
                    keys_guard.push(id);
                }

                // Longer delay for writers (more expensive operations)
                tokio::time::sleep(Duration::from_micros(20)).await;
            }

            println!("Writer {writer_id}: {successful_writes} successful writes");
            successful_writes
        });

        handles.push(handle);
    }

    // Wait for all operations to complete
    let mut total_operations = 0;
    for handle in handles {
        total_operations += handle.await?;
    }

    let duration = start.elapsed();
    let ops_per_second = total_operations as f64 / duration.as_secs_f64();

    println!(
        "Mixed workload: {total_operations} ops in {duration:?} ({ops_per_second:.0} ops/sec)"
    );

    // Verify final tree consistency
    let final_tree = concurrent_tree.read().unwrap();
    let final_keys = shared_keys.read().unwrap();
    let tree_size = count_total_keys(&final_tree);

    assert_eq!(
        tree_size,
        final_keys.len(),
        "Tree size {} doesn't match key count {}",
        tree_size,
        final_keys.len()
    );

    // Performance requirement: should handle at least 1000 ops/sec
    assert!(
        ops_per_second >= 1000.0,
        "Mixed workload performance {ops_per_second:.0} ops/sec below 1000 ops/sec requirement"
    );

    Ok(())
}

/// Test deadlock prevention and lock contention handling
#[tokio::test]
async fn test_deadlock_prevention() -> Result<()> {
    let thread_count = 8;
    let operations_per_thread = 100;

    let tree1 = Arc::new(RwLock::new(btree::create_empty_tree()));
    let tree2 = Arc::new(RwLock::new(btree::create_empty_tree()));
    let mut handles = Vec::new();

    for thread_id in 0..thread_count {
        let tree1_ref = Arc::clone(&tree1);
        let tree2_ref = Arc::clone(&tree2);

        let handle = task::spawn(async move {
            for i in 0..operations_per_thread {
                let id1 = ValidatedDocumentId::from_uuid(Uuid::new_v4()).unwrap();
                let id2 = ValidatedDocumentId::from_uuid(Uuid::new_v4()).unwrap();
                let path1 =
                    ValidatedPath::new(format!("/deadlock/t{thread_id}_op{i}_tree1.md")).unwrap();
                let path2 =
                    ValidatedPath::new(format!("/deadlock/t{thread_id}_op{i}_tree2.md")).unwrap();

                // Alternate lock acquisition order to test deadlock prevention
                if thread_id % 2 == 0 {
                    // Even threads: lock tree1 then tree2
                    let mut guard1 = tree1_ref.write().unwrap();
                    let mut guard2 = tree2_ref.write().unwrap();
                    *guard1 = btree::insert_into_tree(guard1.clone(), id1, path1).unwrap();
                    *guard2 = btree::insert_into_tree(guard2.clone(), id2, path2).unwrap();
                } else {
                    // Odd threads: lock tree2 then tree1
                    let mut guard2 = tree2_ref.write().unwrap();
                    let mut guard1 = tree1_ref.write().unwrap();
                    *guard2 = btree::insert_into_tree(guard2.clone(), id2, path2).unwrap();
                    *guard1 = btree::insert_into_tree(guard1.clone(), id1, path1).unwrap();
                }

                // Small delay to increase contention
                tokio::time::sleep(Duration::from_micros(1)).await;
            }
        });

        handles.push(handle);
    }

    // Use timeout to detect deadlocks
    let timeout_duration = Duration::from_secs(30);
    let start = Instant::now();

    for handle in handles {
        let remaining_time = timeout_duration.saturating_sub(start.elapsed());

        match tokio::time::timeout(remaining_time, handle).await {
            Ok(result) => result?,
            Err(_) => panic!("Deadlock detected: operation timed out after {timeout_duration:?}"),
        }
    }

    // Verify both trees have expected number of entries
    let tree1_size = count_total_keys(&tree1.read().unwrap());
    let tree2_size = count_total_keys(&tree2.read().unwrap());

    assert_eq!(tree1_size, thread_count * operations_per_thread);
    assert_eq!(tree2_size, thread_count * operations_per_thread);

    println!(
        "✅ Deadlock prevention test passed: {} operations completed without deadlock",
        thread_count * operations_per_thread * 2
    );

    Ok(())
}

/// Test lock-free read operations (preparation for optimized concurrent access)
#[tokio::test]
async fn test_lock_free_read_optimization() -> Result<()> {
    // This test prepares for implementing lock-free reads in Stage 3
    // For now, we test read-heavy workloads to establish baseline performance

    let tree_size = 10000;
    let read_threads = 8;
    let reads_per_thread = 2000;

    // Build large tree
    let mut tree = btree::create_empty_tree();
    let mut keys = Vec::new();

    for i in 0..tree_size {
        let id = ValidatedDocumentId::from_uuid(Uuid::new_v4())?;
        let path = ValidatedPath::new(format!("/lockfree/test_{i}.md"))?;
        tree = btree::insert_into_tree(tree, id, path)?;
        keys.push(id);
    }

    // Measure read-only concurrent performance
    let shared_tree = Arc::new(RwLock::new(tree));
    let shared_keys = Arc::new(keys);
    let mut handles = Vec::new();

    let start = Instant::now();

    for _ in 0..read_threads {
        let tree_ref = Arc::clone(&shared_tree);
        let keys_ref = Arc::clone(&shared_keys);

        let handle = task::spawn(async move {
            let mut cache_hits = 0;

            for _ in 0..reads_per_thread {
                let random_key = &keys_ref[fastrand::usize(..keys_ref.len())];
                let tree_guard = tree_ref.read().unwrap();

                if btree::search_in_tree(&tree_guard, random_key).is_some() {
                    cache_hits += 1;
                }
            }

            cache_hits
        });

        handles.push(handle);
    }

    let mut total_hits = 0;
    for handle in handles {
        total_hits += handle.await?;
    }

    let duration = start.elapsed();
    let reads_per_second = (read_threads * reads_per_thread) as f64 / duration.as_secs_f64();

    println!(
        "Lock-free read baseline: {} reads/sec with {} threads",
        reads_per_second as u64, read_threads
    );

    // Performance baseline: should achieve at least 100k reads/sec
    assert!(
        reads_per_second >= 100_000.0,
        "Read performance {reads_per_second:.0} reads/sec below 100k baseline"
    );

    // All reads should succeed (100% hit rate)
    assert_eq!(
        total_hits,
        read_threads * reads_per_thread,
        "Expected {} hits, got {}",
        read_threads * reads_per_thread,
        total_hits
    );

    Ok(())
}

/// Test write batching for improved concurrent write performance
#[tokio::test]
async fn test_write_batching_optimization() -> Result<()> {
    let batch_size = 100;
    let concurrent_writers = 4;
    let batches_per_writer = 10;

    let concurrent_tree = Arc::new(RwLock::new(btree::create_empty_tree()));
    let mut handles = Vec::new();

    let start = Instant::now();

    for writer_id in 0..concurrent_writers {
        let tree_ref = Arc::clone(&concurrent_tree);

        let handle = task::spawn(async move {
            for batch_id in 0..batches_per_writer {
                // Prepare batch
                let mut batch_pairs = Vec::new();
                for i in 0..batch_size {
                    let id = ValidatedDocumentId::from_uuid(Uuid::new_v4()).unwrap();
                    let path =
                        ValidatedPath::new(format!("/batch/w{writer_id}_b{batch_id}_i{i}.md"))
                            .unwrap();
                    batch_pairs.push((id, path));
                }

                // Single lock acquisition for entire batch
                {
                    let mut tree_guard = tree_ref.write().unwrap();
                    // In Stage 3, this will use bulk_insert_into_tree for better performance
                    for (id, path) in batch_pairs {
                        *tree_guard =
                            btree::insert_into_tree(tree_guard.clone(), id, path).unwrap();
                    }
                }

                // Delay between batches
                tokio::time::sleep(Duration::from_millis(1)).await;
            }
        });

        handles.push(handle);
    }

    for handle in handles {
        handle.await?;
    }

    let duration = start.elapsed();
    let total_writes = concurrent_writers * batches_per_writer * batch_size;
    let writes_per_second = total_writes as f64 / duration.as_secs_f64();

    println!("Batched writes: {} writes/sec", writes_per_second as u64);

    // Verify all writes succeeded
    let final_tree = concurrent_tree.read().unwrap();
    let final_size = count_total_keys(&final_tree);
    assert_eq!(
        final_size, total_writes,
        "Expected {total_writes} entries, found {final_size}"
    );

    // Performance target: batching should achieve reasonable throughput
    // This establishes baseline for comparing with bulk operations in Stage 3
    assert!(
        writes_per_second >= 5_000.0,
        "Batched write performance {writes_per_second:.0} writes/sec below 5k baseline"
    );

    Ok(())
}

// Note: These tests define requirements for concurrent access patterns that will be implemented in Stage 3:
// - Read-write locks with optimized concurrent reads
// - Deadlock prevention mechanisms
// - Write batching for improved throughput
// - Lock-free read operations where possible
// - Proper isolation between concurrent operations
//
// The tests establish performance baselines and correctness requirements that the
// Stage 3 implementation must meet or exceed.
