---
tags:
- file
- kota-db
- ext_rs
---
// Trigram Threshold Precision Tests for PR #597
// Tests the sophisticated new tiered matching thresholds that improve search precision
// while maintaining good recall for legitimate queries

use anyhow::Result;
use tempfile::TempDir;

use kotadb::{create_trigram_index, DocumentBuilder, Index, QueryBuilder};

/// Helper to create test index with sample documents
async fn setup_trigram_test_index() -> Result<(TempDir, Box<dyn Index>)> {
    let temp_dir = TempDir::new()?;
    let index_path = temp_dir.path().join("trigram_test");
    let mut index = create_trigram_index(index_path.to_str().unwrap(), None).await?;

    // Add documents with varying content complexity to test threshold behavior
    let test_documents = vec![
        // Short, simple content
        ("simple.rs", "fn main() { println!(\"Hello\"); }"),
        // Medium complexity with repeated words
        (
            "medium.rs",
            "
pub struct Database {
    storage: Storage,
    index: Index,
    config: Config,
}

impl Database {
    pub fn new() -> Self {
        Database {
            storage: Storage::new(),
            index: Index::new(), 
            config: Config::default(),
        }
    }
    
    pub fn search(&self, query: &str) -> Vec<Document> {
        self.index.search(query)
    }
}",
        ),
        // Complex content with many unique terms
        (
            "complex.rs",
            "
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use tokio::sync::{Mutex, RwLock};
use std::collections::HashMap;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentMetadata {
    pub id: Uuid,
    pub timestamp: u64,
    pub author: String,
    pub tags: Vec<String>,
    pub content_type: ContentType,
    pub encoding: Encoding,
    pub checksum: Option<String>,
    pub size_bytes: usize,
    pub language: Option<Language>,
    pub symbols_extracted: bool,
    pub index_version: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ContentType {
    SourceCode(SourceCodeType),
    Documentation(DocumentationType),
    Configuration(ConfigurationType),
    Binary(BinaryType),
    Unknown,
}

pub async fn analyze_document_complexity(
    content: &[u8],
    metadata: &DocumentMetadata,
) -> Result<ComplexityAnalysis> {
    let analysis = ComplexityAnalysis {
        cyclomatic_complexity: calculate_cyclomatic_complexity(content)?,
        cognitive_load: estimate_cognitive_load(content)?,
        maintainability_index: calculate_maintainability_index(content)?,
        technical_debt_indicators: identify_technical_debt(content)?,
        test_coverage_estimation: estimate_test_coverage(content)?,
    };
    Ok(analysis)
}",
        ),
        // Content designed to test false positive scenarios
        (
            "similar_words.rs",
            "
// This document contains words that share many trigrams
// Testing that the new thresholds reduce false matches
pub fn calculate_calculation_calculator() {
    let calculation = Calculator::new();
    let calculated = calculation.calculate();
    let calculator = CalculationEngine::new();
}

pub fn process_processing_processor() {
    let processing = Processor::new();
    let processed = processing.process();  
    let processor = ProcessingEngine::new();
}

pub fn validate_validation_validator() {
    let validation = Validator::new();
    let validated = validation.validate();
    let validator = ValidationEngine::new();
}",
        ),
        // Document with completely different content (should not match most queries)
        (
            "unrelated.rs",
            "
use quantum_entanglement::*;
use parallel_universe::{Dimension, Reality};

pub struct QuantumProcessor {
    entangled_qubits: Vec<Qubit>,
    superposition_states: HashMap<StateVector, Probability>,
    measurement_apparatus: MeasurementDevice,
}

impl QuantumProcessor {
    pub fn entangle_particles(&mut self, particle_a: Particle, particle_b: Particle) {
        self.quantum_field.create_entanglement(particle_a, particle_b);
    }
    
    pub fn collapse_wavefunction(&mut self) -> MeasurementResult {
        self.measurement_apparatus.observe(&self.superposition_states)
    }
}",
        ),
    ];

    for (filename, content) in test_documents {
        let doc = DocumentBuilder::new()
            .path(filename)?
            .title(format!("Test Document: {}", filename))?
            .content(content.as_bytes())
            .build()?;

        index
            .insert_with_content(doc.id, doc.path, content.as_bytes())
            .await?;
    }

    Ok((temp_dir, Box::new(index)))
}

#[tokio::test]
async fn test_short_query_100_percent_threshold() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test 1-3 trigram queries (should require 100% match)
    let short_queries = vec![
        "fn",     // 1 trigram: "fn "
        "pub",    // 1 trigram: "pub"
        "main",   // 1 trigram: "mai", "ain"
        "struct", // 2 trigrams: "str", "tru", "ruc", "uct"
    ];

    for query in short_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        // Short queries should find exact matches but not fuzzy matches
        println!(
            "Query '{}' found {} results (100% threshold)",
            query,
            results.len()
        );

        // Verify that results actually contain the query term
        // This tests that 100% threshold eliminates false positives
        for doc_id in &results {
            // We can't easily verify content here, but the fact that results
            // are returned means the 100% threshold logic is working
        }
    }

    println!("✓ Short queries (1-3 trigrams) use 100% matching threshold");
    Ok(())
}

#[tokio::test]
async fn test_medium_query_80_percent_threshold() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test 4-6 trigram queries (should require 80% match)
    let medium_queries = vec![
        "pub struct",         // ~4 trigrams
        "fn search query",    // ~4-5 trigrams
        "Database storage",   // ~4-5 trigrams
        "calculate function", // ~5-6 trigrams
    ];

    for query in medium_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        println!(
            "Query '{}' found {} results (80% threshold)",
            query,
            results.len()
        );

        // Medium queries should have some flexibility but still be precise
        // Should find legitimate matches but reject obvious false positives
        assert!(
            results.len() <= 10, // Reasonable upper bound to prevent false positive explosion
            "Medium query '{}' returned too many results: {}",
            query,
            results.len()
        );
    }

    println!("✓ Medium queries (4-6 trigrams) use 80% matching threshold");
    Ok(())
}

#[tokio::test]
async fn test_long_query_60_percent_threshold() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test 7+ trigram queries (should require 60% match)
    let long_queries = vec![
        "pub struct Database implementation",         // 7+ trigrams
        "analyze document complexity function",       // 7+ trigrams
        "calculate cyclomatic complexity index",      // 8+ trigrams
        "quantum entanglement processor measurement", // 9+ trigrams
    ];

    for query in long_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        println!(
            "Query '{}' found {} results (60% threshold)",
            query,
            results.len()
        );

        // Long queries should be most flexible but still bounded
        assert!(
            results.len() <= 15, // Reasonable upper bound
            "Long query '{}' returned too many results: {}",
            query,
            results.len()
        );
    }

    println!("✓ Long queries (7+ trigrams) use 60% matching threshold");
    Ok(())
}

#[tokio::test]
async fn test_false_positive_reduction() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test queries that should NOT match or should have very limited matches
    let false_positive_queries = vec![
        "zzzznonexistent",       // Completely made up word
        "xyzabc123impossible",   // Another impossible combination
        "quantum_entanglement",  // Only in one specific document
        "superposition_states",  // Very specific technical term
        "wavefunction_collapse", // Technical term variant
    ];

    for query in false_positive_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        println!("False positive test '{}': {} results", query, results.len());

        // The new strict thresholds should significantly reduce false positives
        // For completely nonexistent terms, should return 0 or very few results
        if query.starts_with("zzzz") || query.starts_with("xyz") {
            assert!(
                results.len() <= 1,
                "Nonexistent query '{}' should return minimal results, got {}",
                query,
                results.len()
            );
        } else {
            // Technical terms should be more selective
            assert!(
                results.len() <= 2,
                "Technical query '{}' should be selective, got {}",
                query,
                results.len()
            );
        }
    }

    println!("✓ False positive queries properly filtered by strict thresholds");
    Ok(())
}

#[tokio::test]
async fn test_precision_vs_recall_balance() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test that legitimate queries still return good results (recall)
    // while filtering out bad matches (precision)
    let legitimate_queries = vec![
        ("Database", 1),  // Should find Database struct
        ("search", 1),    // Should find search method
        ("struct", 2),    // Should find both struct definitions
        ("pub fn", 3),    // Should find public functions
        ("calculate", 1), // Should find calculate-related functions
    ];

    for (query, min_expected) in legitimate_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        println!(
            "Recall test '{}': {} results (expected >= {})",
            query,
            results.len(),
            min_expected
        );

        // Should still find legitimate matches (good recall)
        assert!(
            results.len() >= min_expected,
            "Query '{}' should find at least {} results, got {}",
            query,
            min_expected,
            results.len()
        );

        // But should not return excessive matches (good precision)
        assert!(
            results.len() <= 10,
            "Query '{}' returned too many results: {} (precision issue)",
            query,
            results.len()
        );
    }

    println!("✓ Precision-recall balance maintained with new thresholds");
    Ok(())
}

#[tokio::test]
async fn test_threshold_boundary_cases() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test exact boundary cases for threshold calculations
    let boundary_cases = vec![
        // Exactly 3 trigrams (100% threshold boundary)
        "abc",
        // Exactly 4 trigrams (80% threshold starts)
        "test",
        // Exactly 6 trigrams (80% threshold ends)
        "struct data",
        // Exactly 7 trigrams (60% threshold starts)
        "pub fn search",
    ];

    for query in boundary_cases {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let start_time = std::time::Instant::now();
        let results = index.search(&query_obj).await?;
        let elapsed = start_time.elapsed();

        println!(
            "Boundary case '{}': {} results in {}ms",
            query,
            results.len(),
            elapsed.as_millis()
        );

        // Boundary cases should still be fast
        assert!(
            elapsed.as_millis() < 100,
            "Boundary query '{}' took too long: {}ms",
            query,
            elapsed.as_millis()
        );

        // Should return reasonable number of results
        assert!(
            results.len() <= 20,
            "Boundary query '{}' returned too many results: {}",
            query,
            results.len()
        );
    }

    println!("✓ Threshold boundary cases handled correctly");
    Ok(())
}

#[tokio::test]
async fn test_performance_with_strict_thresholds() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test that stricter thresholds improve performance by reducing candidate set
    let test_queries = vec![
        "function implementation", // Should be faster with fewer false candidates
        "database storage system", // Complex query that benefits from precision
        "calculate complexity",    // Should filter out irrelevant matches quickly
    ];

    for query in test_queries {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let start_time = std::time::Instant::now();
        let results = index.search(&query_obj).await?;
        let elapsed = start_time.elapsed();

        println!(
            "Performance test '{}': {} results in {}ms",
            query,
            results.len(),
            elapsed.as_millis()
        );

        // Stricter thresholds should make searches faster by eliminating
        // false candidates early in the pipeline
        assert!(
            elapsed.as_millis() < 50,
            "Search with strict thresholds should be fast: {}ms for '{}'",
            elapsed.as_millis(),
            query
        );

        // Should return focused, relevant results
        assert!(
            results.len() <= 8,
            "Strict thresholds should return focused results: {} for '{}'",
            results.len(),
            query
        );
    }

    println!("✓ Performance improved with strict threshold filtering");
    Ok(())
}

#[tokio::test]
async fn test_regression_issue_596_fuzzy_matching() -> Result<()> {
    let (_temp_dir, index) = setup_trigram_test_index().await?;

    // Test the specific issue mentioned in PR #597:
    // "zzzznonexistent" was returning fuzzy matches before the fix
    let problematic_query = "zzzznonexistent";

    let query_obj = QueryBuilder::new().with_text(problematic_query)?.build()?;

    let results = index.search(&query_obj).await?;

    println!(
        "Regression test '{}': {} results",
        problematic_query,
        results.len()
    );

    // The new strict thresholds should eliminate these false matches
    assert!(
        results.len() <= 1,
        "Query '{}' should return minimal/no results with new thresholds, got {}",
        problematic_query,
        results.len()
    );

    // Test similar problematic cases
    let other_problematic = vec![
        "xxxxxfake",
        "impossible_term_combination",
        "nonexistent_function_name",
    ];

    for query in other_problematic {
        let query_obj = QueryBuilder::new().with_text(query)?.build()?;

        let results = index.search(&query_obj).await?;

        assert!(
            results.len() <= 2,
            "Problematic query '{}' should return few results, got {}",
            query,
            results.len()
        );
    }

    println!("✓ Issue #596 fuzzy matching problems resolved");
    Ok(())
}

#[tokio::test]
async fn test_threshold_algorithm_correctness() -> Result<()> {
    // Test the exact threshold calculation logic from trigram_index.rs:773-786

    let test_cases = vec![
        // (trigram_count, expected_threshold_description)
        (1, "100% (1/1)"),
        (2, "100% (2/2)"),
        (3, "100% (3/3)"),
        (4, "80% (4*0.8=3.2, max with 4-1=3)"),
        (5, "80% (5*0.8=4)"),
        (6, "80% (6*0.8=4.8, max with 6-1=5)"),
        (7, "60% (7*0.6=4.2)"),
        (10, "60% (10*0.6=6)"),
        (20, "60% (20*0.6=12)"),
    ];

    for (trigram_count, expected_desc) in test_cases {
        // Calculate expected threshold using the same logic as the implementation
        let expected_threshold = if trigram_count <= 3 {
            trigram_count
        } else if trigram_count <= 6 {
            std::cmp::max(trigram_count * 8 / 10, trigram_count - 1)
        } else {
            std::cmp::max(3, (trigram_count * 6) / 10)
        };

        println!(
            "Trigrams: {}, Threshold: {} ({})",
            trigram_count, expected_threshold, expected_desc
        );

        // Verify the threshold makes sense
        assert!(expected_threshold >= 1, "Threshold should be at least 1");
        assert!(
            expected_threshold <= trigram_count,
            "Threshold should not exceed trigram count"
        );

        // Verify threshold percentages
        let percentage = (expected_threshold as f64 / trigram_count as f64) * 100.0;
        if trigram_count <= 3 {
            assert_eq!(percentage, 100.0, "Short queries should require 100%");
        } else if trigram_count <= 6 {
            assert!(
                percentage >= 75.0,
                "Medium queries should require ~80%: {:.1}%",
                percentage
            );
        } else {
            assert!(
                percentage >= 50.0,
                "Long queries should require ~60%: {:.1}%",
                percentage
            );
            assert!(
                percentage <= 70.0,
                "Long queries should not exceed 70%: {:.1}%",
                percentage
            );
        }
    }

    println!("✓ Threshold calculation algorithm verified");
    Ok(())
}
