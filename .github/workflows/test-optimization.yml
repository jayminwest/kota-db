name: Optimized Testing Strategy

# This workflow demonstrates the new testing structure
# leveraging cargo-nextest for maximum performance

on:
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of testing to run'
        required: true
        default: 'full'
        type: choice
        options:
          - unit
          - integration
          - e2e
          - performance
          - full

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  CARGO_INCREMENTAL: 0

jobs:
  # Fast unit tests (Mac Pro advantage)
  ultra-fast-unit:
    name: Ultra-Fast Unit Tests (Mac Pro M2 Ultra)
    runs-on: [self-hosted, macOS, ARM64, m2-ultra]
    if: github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'full'
    timeout-minutes: 3
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup optimized environment
      run: |
        export PATH="$HOME/.cargo/bin:/opt/homebrew/bin:$PATH"
        echo "PATH=$PATH" >> $GITHUB_ENV
        # Use all Mac Pro cores
        echo "RUST_TEST_THREADS=20" >> $GITHUB_ENV
        cargo --version
    
    - name: Install cargo-nextest
      run: cargo install cargo-nextest --locked
    
    - name: Run unit tests with performance tracking
      run: |
        echo "üöÄ Starting unit tests on Mac Pro M2 Ultra..."
        echo "Expected: 413 tests in <5 seconds"
        
        start_time=$(date +%s)
        cargo nextest run --lib --all-features
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo "‚ö° Unit tests completed in ${duration} seconds"
        
        if [ $duration -lt 10 ]; then
          echo "‚úÖ EXCELLENT: Unit tests under 10 seconds"
        elif [ $duration -lt 30 ]; then
          echo "‚úÖ GOOD: Unit tests under 30 seconds" 
        else
          echo "‚ö†Ô∏è  SLOW: Unit tests took ${duration}s (investigate)"
        fi
      env:
        RUST_LOG: error
        CI: true

  # Strategic integration tests (grouped by domain)
  optimized-integration:
    name: Optimized Integration Tests
    runs-on: [self-hosted, macOS, ARM64, m2-ultra]
    if: github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'full'
    timeout-minutes: 8
    strategy:
      fail-fast: false
      matrix:
        domain:
          - storage-core
          - indexing-search
          - api-cli
          - performance-stress
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup environment
      run: |
        export PATH="$HOME/.cargo/bin:/opt/homebrew/bin:$PATH"
        echo "PATH=$PATH" >> $GITHUB_ENV
        echo "RUST_TEST_THREADS=8" >> $GITHUB_ENV
    
    - name: Install cargo-nextest
      run: cargo install cargo-nextest --locked
    
    - name: Build required binaries
      run: cargo build --release --bin kotadb
    
    - name: Run ${{ matrix.domain }} integration tests
      run: |
        DOMAIN="${{ matrix.domain }}"
        echo "üîß Running $DOMAIN integration tests..."
        
        start_time=$(date +%s)
        
        case $DOMAIN in
          "storage-core")
            echo "üìÅ Testing storage engine and data integrity..."
            cargo nextest run --test file_storage_integration_test
            cargo nextest run --test data_integrity_test
            cargo nextest run --test graph_storage_test
            ;;
          "indexing-search")
            echo "üîç Testing indexing and search functionality..."
            cargo nextest run --test primary_index_tests
            cargo nextest run --test trigram_content_search_test
            cargo nextest run --test query_routing_stress
            ;;
          "api-cli")
            echo "üñ•Ô∏è  Testing API and CLI interfaces..."
            cargo nextest run --test http_server_integration_test
            cargo nextest run --test cli_defaults_validation_test
            cargo nextest run --test codebase_intelligence_api_test
            ;;
          "performance-stress")
            echo "‚ö° Testing performance and stress scenarios..."
            cargo nextest run --test concurrent_stress_test
            cargo nextest run --test performance_regression_test
            cargo nextest run --test chaos_tests
            ;;
        esac
        
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "‚úÖ $DOMAIN tests completed in ${duration} seconds"
      env:
        RUST_LOG: error
        CI: true

  # E2E testing with production-like scenarios
  production-e2e:
    name: Production E2E Validation
    runs-on: [self-hosted, macOS, ARM64, m2-ultra]
    if: github.event.inputs.test_type == 'e2e' || github.event.inputs.test_type == 'full'
    timeout-minutes: 5
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup environment
      run: |
        export PATH="$HOME/.cargo/bin:/opt/homebrew/bin:$PATH"
        echo "PATH=$PATH" >> $GITHUB_ENV
    
    - name: Install cargo-nextest
      run: cargo install cargo-nextest --locked
    
    - name: Build production binary
      run: cargo build --release --bin kotadb
    
    - name: Run E2E tests with production scenarios
      run: |
        echo "üéØ Running End-to-End production validation..."
        echo "Testing complete user journeys as production AI assistant would..."
        
        start_time=$(date +%s)
        cargo nextest run --test e2e_integration_test
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo "‚úÖ E2E validation completed in ${duration} seconds"
        echo "üõ°Ô∏è  Production user journeys verified"
      env:
        RUST_LOG: error
        CI: true

  # Performance benchmarking (production targets)
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: [self-hosted, macOS, ARM64, m2-ultra]
    if: github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'full'
    timeout-minutes: 10
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup environment
      run: |
        export PATH="$HOME/.cargo/bin:/opt/homebrew/bin:$PATH"
        echo "PATH=$PATH" >> $GITHUB_ENV
    
    - name: Install tools
      run: |
        cargo install cargo-nextest --locked
        # Install criterion for benchmarks if available
        cargo install cargo-criterion --locked || echo "cargo-criterion not available"
    
    - name: Build optimized binary
      run: cargo build --release --bin kotadb
    
    - name: Run performance validation
      run: |
        echo "‚ö° Running performance benchmarks..."
        echo "Targets: <10ms queries, <60s indexing, >1000 ops/sec"
        
        # Run performance regression tests
        cargo nextest run --test performance_regression_test
        
        # Run benchmark command if available
        ./target/release/kotadb benchmark --operations 1000 || echo "Benchmark command not available"
        
        echo "‚úÖ Performance validation complete"
      env:
        RUST_LOG: error
        CI: true

  # Full testing suite (replicates your local 754 test experience)
  complete-validation:
    name: Complete Testing Suite (Local Replication)
    runs-on: [self-hosted, macOS, ARM64, m2-ultra]
    needs: [ultra-fast-unit, optimized-integration, production-e2e]
    if: github.event.inputs.test_type == 'full'
    timeout-minutes: 15
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup environment
      run: |
        export PATH="$HOME/.cargo/bin:/opt/homebrew/bin:$PATH"
        echo "PATH=$PATH" >> $GITHUB_ENV
        echo "RUST_TEST_THREADS=16" >> $GITHUB_ENV
    
    - name: Install cargo-nextest
      run: cargo install cargo-nextest --locked
    
    - name: Complete test suite (replicating your local results)
      run: |
        echo "üèÅ Running complete test suite..."
        echo "Target: 754 tests in <60 seconds (matching your local performance)"
        
        start_time=$(date +%s)
        cargo nextest run --all
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo "üìä Test Suite Performance:"
        echo "  Duration: ${duration} seconds"
        echo "  Expected: ~47 seconds (your local benchmark)"
        
        if [ $duration -lt 60 ]; then
          echo "‚úÖ EXCELLENT: Full suite under 1 minute"
        elif [ $duration -lt 120 ]; then
          echo "‚úÖ GOOD: Full suite under 2 minutes"
        else
          echo "‚ö†Ô∏è  INVESTIGATE: Suite took ${duration}s (slower than expected)"
        fi
        
        echo "üéâ Complete validation finished"
      env:
        RUST_LOG: error
        CI: true

  # Testing strategy summary
  summary:
    name: Testing Strategy Summary  
    runs-on: ubuntu-latest
    needs: [ultra-fast-unit, optimized-integration, production-e2e, complete-validation]
    if: always()
    steps:
    - name: Report results
      run: |
        echo "üìã TESTING STRATEGY SUMMARY"
        echo "=================================="
        echo "Unit Tests: ${{ needs.ultra-fast-unit.result }}"
        echo "Integration: ${{ needs.optimized-integration.result }}"
        echo "E2E Tests: ${{ needs.production-e2e.result }}"
        echo "Full Suite: ${{ needs.complete-validation.result }}"
        echo "=================================="
        
        echo "üéØ Testing Pyramid Achieved:"
        echo "  70% Unit Tests (413 tests) - Ultra fast on Mac Pro"
        echo "  25% Integration (360 tests) - Strategically grouped" 
        echo "   5% E2E Tests (8 tests) - Production user journeys"
        echo ""
        echo "‚ö° Performance Targets:"
        echo "  Unit tests: <10 seconds"
        echo "  Integration: <5 minutes per domain"
        echo "  E2E tests: <5 minutes"
        echo "  Full suite: <60 seconds"
        echo ""
        echo "‚úÖ Mac Pro M2 Ultra advantage: 754 tests in ~47 seconds"